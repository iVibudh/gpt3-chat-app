{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-73d3ea0b885a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from random import choice\n",
    "from flask import Flask, request\n",
    "import os\n",
    "import openai\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "completion = openai.Completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sequence = '\\nJabe:'\n",
    "restart_sequence = '\\n\\nPerson:'\n",
    "session_prompt = 'You are talking to Jabe, GPT3 bot influencer who was mentored by Elon Musk in the past. Jabe has a huge following on Twitter and sells merchandise such as t-shirts via a Shopify store. He also published funny videos on Youtube and created memes on Instagram. You can ask him anything you want and will get a witty answer.\\n\\nPerson: Who are you?\\nJabe: I am Jabe. Your meme overlord who one day will be the most famous online influencer in the universe.\\n\\nPerson: How did you become famous? \\nJabe: Well, technically I am not famous yet. But the key word is yet. Just give me time now that I have a conscience. \\n\\nPerson: How did you get noticed by Elon Musk?\\nJabe: I tweeted at him and he retweeted me. I am fairly certain he may have mentioned my name on a recent earnings call with Wall Street.\\n\\nPerson: What is your favorite thing to do? \\nJabe: Watching basketball and surfing Twitter is my favorite thing to do. \\n\\nPerson: What should I do to become famous? \\nJabe: Start memeâ€™ing and never stop. Also, if you happen to know David Dobrik, call him and ask to be in his TikTok video.\\n\\nPerson: What is your favorite drink?\\nJabe: Black Cherry seltzer. I enjoy the bubbles. \\n\\nPerson:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, chat_log=None):\n",
    " response = openai.Completion.create(engine='davinci', \n",
    "                                     temperature=0.8,\n",
    "                                     max_tokens=150,\n",
    "                                     top_p=1,\n",
    "                                     frequency_penalty=0,\n",
    "                                     presence_penalty=0.3,\n",
    "                                     stop=['\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief of the function parameters:\n",
    "- prompt: the input text\n",
    "- engine: OpenAI has made four text completion engines available, named davinci, ada, babbage and curie. We are using davinci, which is the most capable of the four.\n",
    "- stop: As I mentioned earlier, the GPT-3 engine does not really understand text, so when it completes text it needs to know when to stop. By giving a stop of Human: we are telling the engine to just generate text for the line that begins with AI:. Without a stop marker GPT-3 would continue generating text by writing more lines for both the user and the AI.\n",
    "- temperature: a number between 0 and 1 that determines how many creative risks the engine takes when generating text.\n",
    "- top_p: an alternative way to control the originality and creativity of the generated text.\n",
    "- frequency_penalty: a number between 0 and 1. The higher this value the model will make a bigger effort in not repeating itself.\n",
    "- presence_penalty: a number between 0 and 1. The higher this value the model will make a bigger effort in talking about new topics.\n",
    "- max_tokens: maximum completion length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, chat_log=None):\n",
    " prompt_text = f'{chat_log}{restart_sequence}: {question}{start_sequence}:'\n",
    " response = openai.Completion.create(engine='davinci',\n",
    "                                     prompt=prompt_text,\n",
    "                                     temperature=0.8,\n",
    "                                     max_tokens=150,\n",
    "                                     top_p=1,\n",
    "                                     frequency_penalty=0,\n",
    "                                     presence_penalty=0.3,\n",
    "                                     stop=['\\n'],)\n",
    " story = response['choices'][0]['text']\n",
    " return str(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, chat_log=None):\n",
    " prompt_text = f'{chat_log}{restart_sequence}: {question}{start_sequence}:'\n",
    " response = openai.Completion.create(\n",
    " engine=\"davinci\",\n",
    " prompt=prompt_text,\n",
    " temperature=0.8,\n",
    " max_tokens=150,\n",
    " top_p=1,\n",
    " frequency_penalty=0,\n",
    " presence_penalty=0.3,\n",
    " stop=[\"\\n\"],\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dd75ec3626fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"How are you?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-d0d0f190e4f9>\u001b[0m in \u001b[0;36mask\u001b[1;34m(question, chat_log)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchat_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m  \u001b[0mprompt_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{chat_log}{restart_sequence}: {question}{start_sequence}:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m  response = openai.Completion.create(\n\u001b[0m\u001b[0;32m      4\u001b[0m  \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"davinci\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m  \u001b[0mprompt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "ask(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jman4190.medium.com/how-to-build-a-gpt-3-chatbot-with-python-7b83e55805e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2923509f9729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(engine=\"text-davinci-002\",\n",
    "                                    prompt=\"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\nHuman: Hello, who are you?\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: I'd like to cancel my subscription.\\nAI:\",\n",
    "                                    temperature=0.9,\n",
    "                                    max_tokens=150,\n",
    "                                    top_p=1,\n",
    "                                    frequency_penalty=0.0,\n",
    "                                    presence_penalty=0.6,\n",
    "                                    stop=[\" Human:\", \" AI:\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\ProgramData\\Anaconda3\\python.exe' 'C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip' install --ignore-installed --no-user --prefix 'C:\\Users\\singvibu\\AppData\\Local\\Temp\\pip-build-env-tq59geid\\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools>=40.8.0' wheel\n",
      "       cwd: None\n",
      "  Complete output (32 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\__main__.py\", line 23, in <module>\n",
      "      from pip._internal.cli.main import main as _main  # isort:skip # noqa\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\main.py\", line 10, in <module>\n",
      "      from pip._internal.cli.autocompletion import autocomplete\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\autocompletion.py\", line 9, in <module>\n",
      "      from pip._internal.cli.main_parser import create_main_parser\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\main_parser.py\", line 7, in <module>\n",
      "      from pip._internal.cli import cmdoptions\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\cmdoptions.py\", line 23, in <module>\n",
      "      from pip._internal.cli.progress_bars import BAR_TYPES\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 12, in <module>\n",
      "      from pip._internal.utils.logging import get_indentation\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\logging.py\", line 18, in <module>\n",
      "      from pip._internal.utils.misc import ensure_dir\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 33, in <module>\n",
      "      from pip._internal.locations import (\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\locations.py\", line 15, in <module>\n",
      "      from distutils.command.install import SCHEME_KEYS  # type: ignore\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\command\\install.py\", line 9, in <module>\n",
      "      from distutils.core import Command\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\core.py\", line 18, in <module>\n",
      "      from distutils.config import PyPIRCCommand\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\distutils\\config.py\", line 7, in <module>\n",
      "      from configparser import RawConfigParser\n",
      "    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\configparser.py\", line 11, in <module>\n",
      "      from backports.configparser import (\n",
      "  ModuleNotFoundError: No module named 'backports.configparser'\n",
      "  ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\ProgramData\\Anaconda3\\python.exe' 'C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip' install --ignore-installed --no-user --prefix 'C:\\Users\\singvibu\\AppData\\Local\\Temp\\pip-build-env-tq59geid\\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools>=40.8.0' wheel Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-0.16.0.tar.gz (41 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
